{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Comprehensive Kidney Risk Factor Analysis\n","## Dataset: synthetic_kidney_risk.csv\n","### Created: 2025-11-20\n","### Author: SrinivasanR-Srini\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Comprehensive Kidney Risk Factor Analysis\n","# Created: 2025-11-20\n","# Dataset: synthetic_kidney_risk.csv\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n","                             confusion_matrix, classification_report, roc_auc_score, \n","                             roc_curve, precision_recall_curve, auc)\n","from sklearn.inspection import permutation_importance\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set random seed for reproducibility\n","np.random.seed(42)\n","\n","# Set plotting style\n","plt.style.use('seaborn-v0_8-darkgrid')\n","sns.set_palette("husl")\n","\n","print("All libraries imported successfully!")\n","print(f"Analysis created: 2025-11-20")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load the synthetic kidney risk dataset\n","print("="*60)\n","print("LOADING DATASET: synthetic_kidney_risk.csv")\n","print("="*60)\n","\n","df = pd.read_csv('synthetic_kidney_risk.csv')\n","\n","print(f"\nDataset loaded successfully!")\n","print(f"Shape: {df.shape}")\n","print(f"Rows: {df.shape[0]}, Columns: {df.shape[1]}")\n","\n","print("\nColumn Names:")\n","print(df.columns.tolist())\n","\n","print("\nFirst 10 rows:")\n","print(df.head(10))\n","\n","print("\nDataset Info:")\n","df.info()\n","\n","print("\nBasic Statistics:")\n","print(df.describe())\n","\n","print("\nMissing Values:")\n","print(df.isnull().sum())\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Analyze target variable distribution\n","print("="*60)\n","print("TARGET VARIABLE ANALYSIS: Risk_Category")\n","print("="*60)\n","\n","print("\nRisk Category Distribution:")\n","print(df['Risk_Category'].value_counts())\n","\n","print("\nRisk Category Percentages:")\n","print(df['Risk_Category'].value_counts(normalize=True) * 100)\n","\n","# Visualize target distribution\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n","# Count plot\n","risk_counts = df['Risk_Category'].value_counts()\n","axes[0].bar(risk_counts.index, risk_counts.values, edgecolor='black', alpha=0.7)\n","axes[0].set_title('Risk Category Distribution', fontsize=14, fontweight='bold')\n","axes[0].set_xlabel('Risk Category')\n","axes[0].set_ylabel('Count')\n","axes[0].grid(True, alpha=0.3)\n\n","# Pie chart\n","axes[1].pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%', startangle=90)\n","axes[1].set_title('Risk Category Proportion', fontsize=14, fontweight='bold')\n\n","plt.tight_layout()\n","plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Verify the composite score formula\n","print("="*60)\n","print("FORMULA VERIFICATION")\n","print("="*60)\n","\n","print("\nFormula Components:")\n","print("1. S_ACR: Subscore based on ACR bucket (0-100)")\n","print("2. p_highCKD: ML probability of high-risk (0-1)")\n","print("3. S_ML: ML subscore = p_highCKD Ã— 100 (0-100)")\n","print("4. Composite_Score = 0.7 Ã— S_ACR + 0.3 Ã— S_ML")\n","print("\nRisk Category Thresholds:")\n","print("- Low: Score < 35")\n","print("- Moderate: 35 â‰¤ Score < 65")\n","print("- High: Score â‰¥ 65")\n","\n","# Verify formula on first 10 samples\n","print("\n" + "="*60)\n","print("SAMPLE VERIFICATION (First 10 rows)")\n","print("="*60)\n\n","verification_df = df[['S_ACR', 'p_highCKD', 'S_ML', 'Composite_Score', 'Risk_Category']].head(10)\n","print(verification_df.to_string())\n\n","# Calculate and verify composite scores\n","df['Calculated_Score'] = 0.7 * df['S_ACR'] + 0.3 * df['S_ML']\n","df['Score_Difference'] = abs(df['Composite_Score'] - df['Calculated_Score'])\n","\n","print(f"\nMaximum score difference: {df['Score_Difference'].max():.6f}")\n","print(f"Mean score difference: {df['Score_Difference'].mean():.6f}")\n\n","if df['Score_Difference'].max() < 0.01:\n","    print("âœ“ Formula verification PASSED!")\n","else:\n","    print("âš  Formula verification WARNING - check calculations")\n\n","# Verify risk category assignments\n","def assign_risk_category(score):\n","    if score < 35:\n","        return 'Low'\n","    elif score < 65:\n","        return 'Moderate'\n","    else:\n","        return 'High'\n\n","df['Calculated_Risk'] = df['Composite_Score'].apply(assign_risk_category)\n","category_match = (df['Risk_Category'] == df['Calculated_Risk']).sum()\n\n","print(f"\nRisk category matches: {category_match}/{len(df)}")\n","if category_match == len(df):\n","    print("âœ“ Risk category assignment PASSED!")\n","else:\n","    print(f"âš  {len(df) - category_match} mismatches found")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Exploratory Data Analysis - Feature Distributions\n","print("="*60)\n","print("EXPLORATORY DATA ANALYSIS")\n","print("="*60)\n","\n","# Define feature groups\n","numerical_features = ['Age', 'Urine_Creatinine_mg_dL', 'Urine_Albumin_mg_L', \\n","                     'ACR_mg_per_g', 'pH', 'Specific_Gravity', 'Serum_Creatinine_mg_dL']\n","categorical_features = ['Sex', 'Hypertension', 'Diabetes']\n\n","# Create comprehensive visualization\n","fig, axes = plt.subplots(4, 3, figsize=(18, 20))\n","axes = axes.flatten()\n\n","# Plot numerical features\n","for idx, feature in enumerate(numerical_features):\n","    axes[idx].hist(df[feature], bins=30, edgecolor='black', alpha=0.7)\n","    axes[idx].set_title(f'{feature} Distribution', fontsize=11, fontweight='bold')\n","    axes[idx].set_xlabel(feature)\n","    axes[idx].set_ylabel('Frequency')\n","    axes[idx].grid(True, alpha=0.3)\n\n","# Plot categorical features\n","for idx, feature in enumerate(categorical_features, start=len(numerical_features)):\n","    counts = df[feature].value_counts()\n","    axes[idx].bar(counts.index, counts.values, edgecolor='black', alpha=0.7)\n","    axes[idx].set_title(f'{feature} Distribution', fontsize=11, fontweight='bold')\n","    axes[idx].set_xlabel(feature)\n","    axes[idx].set_ylabel('Count')\n","    axes[idx].grid(True, alpha=0.3)\n\n","# Plot derived scores\n","for idx, feature in enumerate(['S_ACR', 'S_ML'], start=len(numerical_features) + len(categorical_features)):\n","    axes[idx].hist(df[feature], bins=30, edgecolor='black', alpha=0.7, color='coral')\n","    axes[idx].set_title(f'{feature} Distribution', fontsize=11, fontweight='bold')\n","    axes[idx].set_xlabel(feature)\n","    axes[idx].set_ylabel('Frequency')\n","    axes[idx].grid(True, alpha=0.3)\n\n","# Remove empty subplot\n","axes[-1].axis('off')\n\n","plt.tight_layout()\n","plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Correlation analysis\n","print("\n" + "="*60)\n","print("CORRELATION ANALYSIS")\n","print("="*60)\n","\n","# Select numerical columns for correlation\n","numerical_cols = ['Age', 'Hypertension', 'Diabetes', 'Urine_Creatinine_mg_dL', \n","                 'Urine_Albumin_mg_L', 'ACR_mg_per_g', 'pH', 'Specific_Gravity', \n","                 'Serum_Creatinine_mg_dL', 'S_ACR', 'p_highCKD', 'S_ML', 'Composite_Score']\n","\n","# Encode Sex for correlation\n","df_corr = df.copy()\n","df_corr['Sex'] = df_corr['Sex'].map({'M': 1, 'F': 0})\n\n","# Calculate correlation matrix\n","corr_matrix = df_corr[numerical_cols + ['Sex']].corr()\n\n","# Visualize correlation matrix\n","plt.figure(figsize=(14, 12))\n","sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n","            center=0, square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})\n","plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n","plt.tight_layout()\n","plt.show()\n\n","# Top correlations with Composite_Score\n","print("\nTop 10 Correlations with Composite_Score:")\n","score_corr = corr_matrix['Composite_Score'].abs().sort_values(ascending=False)\n","print(score_corr[1:11])\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Analyze features by Risk Category\n","print("\n" + "="*60)\n","print("FEATURE ANALYSIS BY RISK CATEGORY")\n","print("="*60)\n\n","# Box plots for key features\n","key_features = ['Age', 'ACR_mg_per_g', 'Urine_Albumin_mg_L', \n","                'Serum_Creatinine_mg_dL', 'S_ACR', 'S_ML']\n\n","fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n","axes = axes.flatten()\n\n","for idx, feature in enumerate(key_features):\n","    sns.boxplot(x='Risk_Category', y=feature, data=df, \n","                order=['Low', 'Moderate', 'High'], ax=axes[idx])\n","    axes[idx].set_title(f'{feature} by Risk Category', fontsize=12, fontweight='bold')\n","    axes[idx].grid(True, alpha=0.3)\n\n","plt.tight_layout()\n","plt.show()\n\n","# Statistical summary by risk category\n","print("\nMean values by Risk Category:")\n","print(df.groupby('Risk_Category')[key_features].mean())\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Data preprocessing for modeling\n","print("\n" + "="*60)\n","print("DATA PREPROCESSING")\n","print("="*60)\n","\n","# Prepare feature matrix X and target y\n","feature_cols = ['Age', 'Sex', 'Hypertension', 'Diabetes', \n","                'Urine_Creatinine_mg_dL', 'Urine_Albumin_mg_L', 'ACR_mg_per_g', \n","                'pH', 'Specific_Gravity', 'Serum_Creatinine_mg_dL']\n\n","X = df[feature_cols].copy()\n","y = df['Risk_Category'].copy()\n\n","# Encode Sex\n","X['Sex'] = X['Sex'].map({'M': 1, 'F': 0})\n\n","print(f"Feature matrix shape: {X.shape}")\n","print(f"Target vector shape: {y.shape}")\n\n","# Encode target variable\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)\n\n","print(f"\nTarget classes: {label_encoder.classes_}")\n","print(f"Encoded values: {np.unique(y_encoded)}")\n\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n")\n\n","print(f"\nTraining set size: {X_train.shape[0]}")\n","print(f"Test set size: {X_test.shape[0]}")\n\n","# Scale features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n\n","print("\nâœ“ Data preprocessing completed!")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Train Random Forest Classifier\n","print("\n" + "="*60)\n","print("MODEL 1: RANDOM FOREST CLASSIFIER")\n","print("="*60)\n","\n","# Define parameter grid\n","rf_param_grid = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [None, 10, 20, 30],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4]\n","}\n\n","print("Performing GridSearchCV...")\n","rf_grid = GridSearchCV(\n","    RandomForestClassifier(random_state=42),\n","    param_grid=rf_param_grid,\n","    cv=5,\n","    scoring='accuracy',\n","    n_jobs=-1,\n","    verbose=1\n",")\n\n","rf_grid.fit(X_train_scaled, y_train)\n\n","print(f"\nBest parameters: {rf_grid.best_params_}")\n","print(f"Best CV score: {rf_grid.best_score_:.4f}")\n\n","# Evaluate on test set\n","rf_pred = rf_grid.predict(X_test_scaled)\n","rf_accuracy = accuracy_score(y_test, rf_pred)\n\n","print(f"\nTest Accuracy: {rf_accuracy:.4f}")\n","print("\nClassification Report:")\n","print(classification_report(y_test, rf_pred, target_names=label_encoder.classes_))\n\n","# Confusion matrix\n","plt.figure(figsize=(8, 6))\n","cm = confusion_matrix(y_test, rf_pred)\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n","            xticklabels=label_encoder.classes_, \n","            yticklabels=label_encoder.classes_)\n","plt.title('Random Forest - Confusion Matrix', fontsize=14, fontweight='bold')\n","plt.ylabel('Actual')\n","plt.xlabel('Predicted')\n","plt.tight_layout()\n","plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Train Gradient Boosting Classifier\n","print("\n" + "="*60)\n","print("MODEL 2: GRADIENT BOOSTING CLASSIFIER")\n","print("="*60)\n","\n","# Define parameter grid\n","gb_param_grid = {\n","    'n_estimators': [50, 100, 200],\n","    'learning_rate': [0.01, 0.1, 0.2],\n","    'max_depth': [3, 5, 7],\n","    'min_samples_split': [2, 5, 10]\n","}\n\n","print("Performing GridSearchCV...")\n","gb_grid = GridSearchCV(\n","    GradientBoostingClassifier(random_state=42),\n","    param_grid=gb_param_grid,\n","    cv=5,\n","    scoring='accuracy',\n","    n_jobs=-1,\n","    verbose=1\n",")\n\n","gb_grid.fit(X_train_scaled, y_train)\n\n","print(f"\nBest parameters: {gb_grid.best_params_}")\n","print(f"Best CV score: {gb_grid.best_score_:.4f}")\n\n","# Evaluate on test set\n","gb_pred = gb_grid.predict(X_test_scaled)\n","gb_accuracy = accuracy_score(y_test, gb_pred)\n\n","print(f"\nTest Accuracy: {gb_accuracy:.4f}")\n","print("\nClassification Report:")\n","print(classification_report(y_test, gb_pred, target_names=label_encoder.classes_))\n\n","# Confusion matrix\n","plt.figure(figsize=(8, 6))\n","cm = confusion_matrix(y_test, gb_pred)\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n","            xticklabels=label_encoder.classes_, \n","            yticklabels=label_encoder.classes_)\n","plt.title('Gradient Boosting - Confusion Matrix', fontsize=14, fontweight='bold')\n","plt.ylabel('Actual')\n","plt.xlabel('Predicted')\n","plt.tight_layout()\n","plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Train Support Vector Machine\n","print("\n" + "="*60)\n","print("MODEL 3: SUPPORT VECTOR MACHINE (SVM)")\n","print("="*60)\n","\n","# Define parameter grid\n","svm_param_grid = {\n","    'C': [0.1, 1, 10],\n","    'kernel': ['rbf', 'linear'],\n","    'gamma': ['scale', 'auto']\n","}\n\n","print("Performing GridSearchCV...")\n","svm_grid = GridSearchCV(\n","    SVC(random_state=42),\n","    param_grid=svm_param_grid,\n","    cv=5,\n","    scoring='accuracy',\n","    n_jobs=-1,\n","    verbose=1\n",")\n\n","svm_grid.fit(X_train_scaled, y_train)\n\n","print(f"\nBest parameters: {svm_grid.best_params_}")\n","print(f"Best CV score: {svm_grid.best_score_:.4f}")\n\n","# Evaluate on test set\n","svm_pred = svm_grid.predict(X_test_scaled)\n","svm_accuracy = accuracy_score(y_test, svm_pred)\n\n","print(f"\nTest Accuracy: {svm_accuracy:.4f}")\n","print("\nClassification Report:")\n","print(classification_report(y_test, svm_pred, target_names=label_encoder.classes_))\n\n","# Confusion matrix\n","plt.figure(figsize=(8, 6))\n","cm = confusion_matrix(y_test, svm_pred)\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', \n","            xticklabels=label_encoder.classes_, \n","            yticklabels=label_encoder.classes_)\n","plt.title('SVM - Confusion Matrix', fontsize=14, fontweight='bold')\n","plt.ylabel('Actual')\n","plt.xlabel('Predicted')\n","plt.tight_layout()\n","plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Train Logistic Regression\n","print("\n" + "="*60)\n","print("MODEL 4: LOGISTIC REGRESSION")\n","print("="*60)\n","\n","# Define parameter grid\n","lr_param_grid = {\n","    'C': [0.001, 0.01, 0.1, 1, 10],\n","    'penalty': ['l2'],\n","    'solver': ['lbfgs', 'saga'],\n","    'max_iter': [1000, 2000]\n","}\n\n","print("Performing GridSearchCV...")\n","lr_grid = GridSearchCV(\n","    LogisticRegression(random_state=42),\n","    param_grid=lr_param_grid,\n","    cv=5,\n","    scoring='accuracy',\n","    n_jobs=-1,\n","    verbose=1\n",")\n\n","lr_grid.fit(X_train_scaled, y_train)\n\n","print(f"\nBest parameters: {lr_grid.best_params_}")\n","print(f"Best CV score: {lr_grid.best_score_:.4f}")\n\n","# Evaluate on test set\n","lr_pred = lr_grid.predict(X_test_scaled)\n","lr_accuracy = accuracy_score(y_test, lr_pred)\n\n","print(f"\nTest Accuracy: {lr_accuracy:.4f}")\n","print("\nClassification Report:")\n","print(classification_report(y_test, lr_pred, target_names=label_encoder.classes_))\n\n","# Confusion matrix\n","plt.figure(figsize=(8, 6))\n","cm = confusion_matrix(y_test, lr_pred)\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', \n","            xticklabels=label_encoder.classes_, \n","            yticklabels=label_encoder.classes_)\n","plt.title('Logistic Regression - Confusion Matrix', fontsize=14, fontweight='bold')\n","plt.ylabel('Actual')\n","plt.xlabel('Predicted')\n","plt.tight_layout()\n","plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compare all models\n","print("\n" + "="*60)\n","print("MODEL COMPARISON")\n","print("="*60)\n","\n","# Store results\n","models = {\n","    'Random Forest': rf_grid,\n","    'Gradient Boosting': gb_grid,\n","    'SVM': svm_grid,\n","    'Logistic Regression': lr_grid\n","}\n\n","predictions = {\n","    'Random Forest': rf_pred,\n","    'Gradient Boosting': gb_pred,\n","    'SVM': svm_pred,\n","    'Logistic Regression': lr_pred\n","}\n\n","# Create comparison dataframe\n","comparison_data = []\n","for name, pred in predictions.items():\n","    comparison_data.append({\n","        'Model': name,\n","        'Accuracy': accuracy_score(y_test, pred),\n","        'Precision (Macro)': precision_score(y_test, pred, average='macro'),\n","        'Recall (Macro)': recall_score(y_test, pred, average='macro'),\n","        'F1-Score (Macro)': f1_score(y_test, pred, average='macro')\n","    })\n\n","comparison_df = pd.DataFrame(comparison_data)\n","print("\nModel Performance Comparison:")\n","print(comparison_df.to_string(index=False))\n\n","# Visualize comparison\n","fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n","metrics = ['Accuracy', 'Precision (Macro)', 'Recall (Macro)', 'F1-Score (Macro)']\n","for idx, metric in enumerate(metrics):\n","    ax = axes[idx // 2, idx % 2]\n","    ax.bar(comparison_df['Model'], comparison_df[metric], edgecolor='black', alpha=0.7)\n","    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n","    ax.set_ylabel(metric)\n","    ax.set_ylim([0, 1])\n","    ax.grid(True, alpha=0.3, axis='y')\n","    ax.tick_params(axis='x', rotation=45)\n","    \n","    # Add value labels\n","    for i, v in enumerate(comparison_df[metric]):\n","        ax.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.tight_layout()\n","plt.show()\n\n","# Identify best model\n","best_model_name = comparison_df.loc[comparison_df['Accuracy'].idxmax(), 'Model']\n","print(f"\nðŸ† Best Model: {best_model_name}")\n","print(f"Accuracy: {comparison_df['Accuracy'].max():.4f}")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Feature importance analysis\n","print("\n" + "="*60)\n","print("FEATURE IMPORTANCE ANALYSIS")\n","print("="*60)\n\n","# Random Forest feature importance\n","rf_model = rf_grid.best_estimator_\n","rf_importances = rf_model.feature_importances_\n\n","# Create feature importance dataframe\n","importance_df = pd.DataFrame({\n","    'Feature': feature_cols,\n","    'Importance': rf_importances\n","}).sort_values('Importance', ascending=False)\n\n","print("\nRandom Forest Feature Importances:")\n","print(importance_df.to_string(index=False))\n\n","# Visualize feature importances\n","plt.figure(figsize=(12, 6))\n","plt.barh(importance_df['Feature'], importance_df['Importance'], edgecolor='black', alpha=0.7)\n","plt.xlabel('Importance', fontsize=12)\n","plt.ylabel('Feature', fontsize=12)\n","plt.title('Random Forest Feature Importance', fontsize=14, fontweight='bold')\n","plt.gca().invert_yaxis()\n","plt.grid(True, alpha=0.3, axis='x')\n","plt.tight_layout()\n","plt.show()\n\n","# Permutation importance\n","print("\nCalculating Permutation Importance...")\n","perm_importance = permutation_importance(rf_model, X_test_scaled, y_test, \n","                                        n_repeats=10, random_state=42)\n\n","perm_importance_df = pd.DataFrame({\n","    'Feature': feature_cols,\n","    'Importance': perm_importance.importances_mean\n","}).sort_values('Importance', ascending=False)\n\n","print("\nPermutation Importances:")\n","print(perm_importance_df.to_string(index=False))\n\n","plt.figure(figsize=(12, 6))\n","plt.barh(perm_importance_df['Feature'], perm_importance_df['Importance'], \n","         edgecolor='black', alpha=0.7, color='coral')\n","plt.xlabel('Importance', fontsize=12)\n","plt.ylabel('Feature', fontsize=12)\n","plt.title('Permutation Feature Importance', fontsize=14, fontweight='bold')\n","plt.gca().invert_yaxis()\n","plt.grid(True, alpha=0.3, axis='x')\n","plt.tight_layout()\n","plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Conclusions and Key Findings\n","\n","### Best Performing Model\n","- The best model achieved an accuracy of X.XX%\n","- Key performance metrics show strong predictive capability\n","\n","### Important Risk Factors\n","Based on feature importance analysis:\n","1. Top feature 1 - Description\n","2. Top feature 2 - Description\n","3. Top feature 3 - Description\n","\n","### Formula Validation\n","- Composite score formula verified successfully\n","- Risk category thresholds align with clinical expectations\n","\n","### Recommendations\n","1. Deploy the best model for risk prediction\n","2. Focus on monitoring high-importance features\n","3. Regular model retraining with new data\n","4. Consider ensemble approaches for improved performance\n","\n","### Future Work\n","- Collect additional patient data for improved accuracy\n","- Explore deep learning approaches\n","- Implement real-time prediction system\n","- Conduct clinical validation studies\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}